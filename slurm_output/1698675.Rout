
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

- Project '/oscar/home/akhann16/code/net-ergm-v4plus' loaded. [renv 1.0.3]
> # Analyze synthetic dataset with 32K nodes 
> 
> # Fit ERGM with 5 dyadic independent terms
> 
> rm(list=ls())
> 
> # Activate R environment ------------------------------
> 
> library(renv)

Attaching package: ‘renv’

The following objects are masked from ‘package:stats’:

    embed, update

The following objects are masked from ‘package:utils’:

    history, upgrade

The following objects are masked from ‘package:base’:

    autoload, load, remove

> renv::activate()
> 
> 
> # Libraries ----------
> 
> library(network)

‘network’ 1.18.2 (2023-12-04), part of the Statnet Project
* ‘news(package="network")’ for changes since last version
* ‘citation("network")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

> library(ergm)

‘ergm’ 4.6.0 (2023-12-17), part of the Statnet Project
* ‘news(package="ergm")’ for changes since last version
* ‘citation("ergm")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

‘ergm’ 4 is a major update that introduces some backwards-incompatible
changes. Please type ‘news(package="ergm")’ for a list of major
changes.

> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> library(ergm.userterms)
Loading required package: statnet.common

Attaching package: ‘statnet.common’

The following objects are masked from ‘package:base’:

    attr, order


‘ergm.userterms’ 3.1.1 (2020-02-01), part of the Statnet Project
* ‘news(package="ergm.userterms")’ for changes since last version
* ‘citation("ergm.userterms")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

NOTE: If you use custom ERGM terms based on ‘ergm.userterms’ version
prior to 3.1, you will need to perform a one-time update of the package
boilerplate files (the files that you did not write or modify) from
‘ergm.userterms’ 3.1 or later. See help('eut-upgrade') for
instructions.

> library(here)
here() starts at /oscar/home/akhann16/code/net-ergm-v4plus
> 
> 
> # Read Synthpop Data ------------------------------ 
> 
> data_path <- here("data", "synthpop-2022-07-25 13_21_04.csv")
> data <- read.csv(data_path, header=TRUE)
> glimpse(data)
Rows: 32,001
Columns: 15
$ sex                       <chr> "M", "M", "M", "M", "M", "M", "M", "M", "M",…
$ race                      <chr> "Wh", "Wh", "Wh", "Wh", "Wh", "Wh", "Wh", "W…
$ agecat                    <chr> "18-24", "18-24", "18-24", "18-24", "18-24",…
$ age_started               <int> 18, 18, 16, 17, 19, 19, 20, 18, 18, 22, 16, …
$ fraction_recept_sharing   <dbl> 1.00, 0.75, 0.75, 0.50, 0.00, 0.00, 0.00, 1.…
$ syringe_source            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…
$ daily_injection_intensity <chr> "3x/week or less", "3x/week or less", "once …
$ age_lb                    <int> 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, …
$ age_ub                    <int> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, …
$ age                       <int> 21, 21, 22, 22, 23, 24, 20, 18, 24, 22, 24, …
$ zipcode                   <int> 60644, 60644, 60644, 60644, 60644, 60644, 60…
$ Zip                       <int> 60644, 60644, 60644, 60644, 60644, 60644, 60…
$ lon                       <dbl> -87.77475, -87.75076, -87.75811, -87.75109, …
$ lat                       <dbl> 41.89089, 41.89182, 41.87218, 41.87696, 41.8…
$ hcv_status                <chr> "sucseptible", "sucseptible", "sucseptible",…
> 
> # Input  In/Outdegree Data ------------------------------
> # (from networks simulated form the fit, as an alternate source of targets)
> 
> inedges <- read.csv(here("data", "pplrss.csv")) #in- and out-edges
> outedges <- read.csv(here("data", "ppldss.csv"))
> negbin_indeg <- read.csv(here("data", "negbin-indeg.csv"))
> negbin_outdeg <- read.csv(here("data", "negbin-outdeg.csv"))
> inedges.data <- read.csv(file = here("data", "inedges_data.csv"), header = TRUE)
> outedges.data <- read.csv(file = here("data", "outedges_data.csv"), header = TRUE)
> 
> # Input  Degree Distribution Data ------------------------------
> 
> #degree_distribution_target_stats <- 
>  # readRDS(here("simulate-from-ergms/out/degree_distribution_target_stats.RDS"))
> #names(degree_distribution_target_stats)
> 
>  
> 
> # Initialize network ----------
> 
> n <- nrow(data)
> n0 <- network.initialize(n = n, directed = TRUE)
> 
> 
> # Compute target number of edges ----------
> 
> inedges_target <- sum(inedges.data$k * inedges.data$nbprob * n)
> outedges_target <- sum(outedges.data$k * outedges.data$nbprob * n)
> edges_target <- mean(c(inedges_target, outedges_target))
> 
> 
> 
> 
> # Assign vertex attributes to the network ----------
> 
> synthpop_cols <- colnames(data)
> set.vertex.attribute(n0, colnames(data), data)
> 
> 
> # Test assignment between `n0` vertex attributes and `synthpop_cols` ----------
> 
> ## For sex variable
> identical(n0%v%"sex", data$sex)
[1] TRUE
> table(n0%v%"sex", exclude=NULL)

    F     M 
10476 21525 
> 
> ## For all variables:
> ### get vertex attribute names from the n0 graph object
> vertex_attr_names <- list.vertex.attributes(n0)
> 
> # define a function to check if a vertex attribute is identical to the corresponding column in data
> check_identical <- function(attr_name) {
+   graph_attr <- n0 %v% attr_name
+   data_attr <- data[[attr_name]]
+   
+   return(identical(graph_attr, data_attr))
+ }
> 
> ### use sapply() to apply the check_identical() function to all vertex attribute names
> identical_results <- sapply(vertex_attr_names, check_identical)
> 
> ### print the results
> print(identical_results)
                      age                    age_lb               age_started 
                     TRUE                      TRUE                      TRUE 
                   age_ub                    agecat daily_injection_intensity 
                     TRUE                      TRUE                      TRUE 
  fraction_recept_sharing                hcv_status                       lat 
                     TRUE                      TRUE                      TRUE 
                      lon                        na                      race 
                     TRUE                     FALSE                      TRUE 
                      sex            syringe_source              vertex.names 
                     TRUE                      TRUE                     FALSE 
                      Zip                   zipcode 
                     TRUE                      TRUE 
> 
> ### check if any values other than 'vertex.names' are FALSE
> if (any(identical_results[!(names(identical_results) %in% c("vertex.names", "na"))] == FALSE)) {
+   stop("Error: Some vertex attributes do not match the corresponding columns in data.")
+ } else {
+   cat("Test passed: All relevant vertex attributes match the corresponding columns in data.\n")
+ }
Test passed: All relevant vertex attributes match the corresponding columns in data.
> 
> 
> # Recode to add new variables to dataset ------------------------------
> 
> # young (< 26, to be set = 1) vs old (>= 26, to be set = 0) 
> age.cat <- n0 %v% "age"
> age.cat.df <- as.data.frame(age.cat)
> age.cat.df <-  
+   mutate(age.cat, young = ifelse(age.cat <26, 1, 0), .data = age.cat.df)
> xtabs(~age.cat+young, data = age.cat.df)
       young
age.cat    0    1
     18    0  813
     19    0 1147
     20    0 1387
     21    0 1784
     22    0 1969
     23    0 2118
     24    0 2573
     25    0  739
     26  792    0
     27  947    0
     28  941    0
     29  985    0
     30 1006    0
     31 1058    0
     32 1098    0
     33 1134    0
     34 1082    0
     35  382    0
     36  347    0
     37  383    0
     38  413    0
     39  414    0
     40  433    0
     41  445    0
     42  444    0
     43  454    0
     44  456    0
     45  182    0
     46  180    0
     47  171    0
     48  194    0
     49  181    0
     50  163    0
     51  161    0
     52  174    0
     53  171    0
     54  169    0
     55  154    0
     56  168    0
     57  175    0
     58  193    0
     59  168    0
     60  164    0
     61  175    0
     62  175    0
     63  180    0
     64  183    0
     65  189    0
     66  170    0
     67  176    0
     68  159    0
     69  162    0
     70  171    0
     71  154    0
     72  147    0
     73  186    0
     74  181    0
     75  197    0
     76  206    0
     77  167    0
     78  198    0
     79  147    0
     80  166    0
> 
> # recode race variable to set ordering of categories
> race <- n0 %v% "race"
> race.num <- recode(race, 
+                    Wh = 1, Bl = 2,
+                    Hi = 3, Ot = 4)
> 
> # Initialize network and add attributes ----------
> 
> n0 %v% "young" <- age.cat.df$young
> n0 %v% "race.num" <- race.num
> 
> 
> # Generate target statistics from meta-mixing data ----------
> 
> ## gender 
> ### mixing information from meta-analysis of sathcap AND socnet
> edges.male.end <- mean(c(0.58, 0.59))
> edges.female.end <- mean(c(0.40, 0.41))
> 
> male.pctmale <- 0.53 
> male.pctfemale <- 0.47
> female.pctmale <- 0.75
> female.pctfemale <- 0.22
> 
> ### set gender targets 
> tgt.male.pctmale <- edges_target*edges.male.end*male.pctmale
> tgt.male.pctfemale <- edges_target*edges.male.end*male.pctfemale
> tgt.female.pctmale <- edges_target*edges.female.end*female.pctmale  
> tgt.female.pctfemale <- edges_target*edges.female.end*female.pctfemale
> 
> ## young (1=young; 0=old)
> ### mixing information from meta-analysis of sathcap AND socnet
> edges.young.end <- 0.10
> edges.old.end <- 0.90
>   
> young.pctyoung <- 0.60
> young.pctold <- 0.40
> old.pctyoung <- 0.14
> old.pctold <- 0.86
>  
> ## set young targets from meta data
> tgt.young.pctyoung <- edges_target * edges.young.end * young.pctyoung
> tgt.young.pctold <- edges_target * edges.young.end * young.pctold
> tgt.old.pctyoung <- edges_target * edges.old.end * old.pctyoung
> tgt.old.pctold <- edges_target * edges.old.end * old.pctold
> 
> 
> ## chicago
> ### mixing information from meta-analysis of sathcap AND socnet
> 
> edges.chicago.end <- 0.87
> edges.nonchicago.end <- 0.13
> 
> chicago.pctchicago <- 0.67
> chicago.pctnonchicago <- 0.30
> nonchicago.pctchicago <- 0.60
> nonchicago.pctnonchicago <- 0.40
> 
> 
> # ### mixing from simulation
> # chicago.mm <- mixingmatrix(sim, "chicago") #fem=1, chicago=2
> # from.nonchicago <- sum(chicago.mm$matrix[,1])
> # from.chicago <- sum(chicago.mm$matrix[,2])
> # 
> # ### set chicago targets from sathcap
> # tgt.chicago.pctchicago <- from.chicago*chicago.pctchicago
> # tgt.chicago.pctnonchicago <- from.chicago*chicago.pctnonchicago
> # tgt.nonchicago.pctchicago <- from.nonchicago*chicago.pctchicago
> # tgt.nonchicago.pctnonchicago <- from.nonchicago*chicago.pctnonchicago
> 
> 
> ## race (1=Black, 2=hispani,3=other, 4=white)
> 
> table(n0 %v% "race.num", exclude=NULL) # will be sorted as per 1=W, 2=B, 3=H, 4=O

    1     2     3     4 
16048  7168  7592  1193 
> 
> pct_to_white	<- mean(c(0.30, 0.31))
> pct_to_black	<- mean(c(0.41,	0.42))
> pct_to_hispanic	<- mean(c(0.21, 0.22))
> pct_to_other	<- mean(c(0.04,	0.05))
> 
> race.w.w <- 0.73
> race.b.w <- 0.10
> race.h.w <- 0.12
> race.o.w <- 0.04
> race.w.b <- 0.10
> race.b.b <- 0.83
> race.h.b <- 0.06
> race.o.b <- 0.01
> race.w.h <- 0.21
> race.b.h <- 0.15
> race.h.h <- 0.62
> race.o.h <- 0.02
> race.w.o <- 0.43
> race.b.o <- 0.21
> race.h.o <- 0.22
> race.o.o <- 0.14
> 
> target.w.w <- edges_target * pct_to_white * race.w.w
> target.b.w <- edges_target * pct_to_white * race.b.w
> target.h.w <- edges_target * pct_to_white * race.h.w
> target.o.w <- edges_target * pct_to_white * race.o.w
> 
> target.w.b <- edges_target * pct_to_black * race.w.b
> target.b.b <- edges_target * pct_to_black * race.b.b
> target.h.b <- edges_target * pct_to_black * race.h.b
> target.o.b <- edges_target * pct_to_black * race.o.b
> 
> target.w.h <- edges_target * pct_to_hispanic * race.w.h
> target.b.h <- edges_target * pct_to_hispanic * race.b.h
> target.h.h <- edges_target * pct_to_hispanic * race.h.h
> target.o.h <- edges_target * pct_to_hispanic * race.o.h
> 
> target.w.o <- edges_target * pct_to_other * race.w.o
> target.b.o <- edges_target * pct_to_other * race.b.o
> target.h.o <- edges_target * pct_to_other * race.h.o
> target.o.o <- edges_target * pct_to_other * race.o.o
> 
> ## degree distributions
> inedges <- inedges %>% 
+   mutate(n_nodes = n*nbprob)
> outedges <- outedges %>% 
+   mutate(n_nodes = n*nbprob)
> 
> negbin_inedges <- negbin_indeg %>% 
+   mutate(n_nodes = n*nbprob)
> negbin_outedges <- negbin_outdeg %>% 
+   mutate(n_nodes = n*nbprob)
> 
> ## distance term
> 
> dist.prop.distribution <- c(15.7, 35.1, 24.1, 22)/100
> dist.nedge.distribution <- edges_target*dist.prop.distribution
> 
> # Fit ERGM (with SATHCAP mixing) ----------
> 
> deg.terms <- 0:3
> indeg.terms <- 0:1  
> 
> dist.terms <- 1:3 #fourth is left out
> 
> 
> fit.metadata.mixing <-
+   ergm(
+     n0 ~
+       edges+
+       nodemix("sex", levels2=-1)+
+       nodemix("young", levels2=-1)+
+       nodemix("race.num", levels2=-1)+
+       idegree(indeg.terms)+
+       odegree(deg.terms)+
+       dist(dist.terms),
+     target.stats = 
+     c(
+       edges_target,
+       c(tgt.female.pctmale, tgt.male.pctfemale, tgt.male.pctmale),           
+       c(tgt.old.pctyoung, tgt.young.pctold, tgt.young.pctyoung),
+       c(target.b.w, target.h.w, target.o.w,
+         target.w.b, target.b.b, target.h.b, target.o.b,
+         target.w.h, target.b.h, target.h.h, target.o.h,
+         target.w.o, target.b.o, target.h.o, target.o.o),
+       c(negbin_inedges$n_nodes[c(indeg.terms+1)]),
+       c(outedges$n_nodes[c(deg.terms+1)]),
+       c(dist.nedge.distribution[dist.terms])
+     ),
+     eval.loglik = FALSE,
+     control = control.ergm(MCMLE.maxit = 500,
+                            MCMC.interval = 1e5,
+                            MCMC.samplesize = 1e5,
+                            #checkpoint="step_%04d.RData",
+                            #MCMLE.density.guard = 1e4, 
+                            SAN = control.san(
+                              SAN.maxit = 500, 
+                              SAN.nsteps = 1e8
+                              #SAN.nsteps.times = 16
+                            )
+                            
+     )
+   )
Unable to match target stats. Using MCMLE estimation.
Starting maximum pseudolikelihood estimation (MPLE):
Obtaining the responsible dyads.
Evaluating the predictor and response matrix.
Maximizing the pseudolikelihood.
Finished MPLE.
Starting Monte Carlo maximum likelihood estimation (MCMLE):
Iteration 1 of at most 500:
Optimizing with step length 0.0178.
The log-likelihood improved by 6.0469.
Estimating equations are not within tolerance region.
Iteration 2 of at most 500:
Optimizing with step length 0.0169.
The log-likelihood improved by 5.2685.
Estimating equations are not within tolerance region.
Iteration 3 of at most 500:
Optimizing with step length 0.0155.
The log-likelihood improved by 4.8328.
Estimating equations are not within tolerance region.
Iteration 4 of at most 500:
Optimizing with step length 0.0188.
The log-likelihood improved by 5.2285.
Estimating equations are not within tolerance region.
Iteration 5 of at most 500:
Optimizing with step length 0.0190.
The log-likelihood improved by 5.1148.
Estimating equations are not within tolerance region.
Iteration 6 of at most 500:
Optimizing with step length 0.0196.
The log-likelihood improved by 4.6811.
Estimating equations are not within tolerance region.
Iteration 7 of at most 500:
