
R version 4.4.0 (2024-04-24) -- "Puppy Cup"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

- Project '/oscar/home/akhann16/code/net-ergm-v4plus' loaded. [renv 1.0.7]
> # Analyze synthetic dataset with 32K nodes 
> 
> # Fit ERGM with 5 dyadic independent terms
> 
> rm(list=ls())
> 
> # Activate R environment ------------------------------
> 
> library(renv)

Attaching package: ‘renv’

The following objects are masked from ‘package:stats’:

    embed, update

The following objects are masked from ‘package:utils’:

    history, upgrade

The following objects are masked from ‘package:base’:

    autoload, load, remove, use

> renv::activate()
> 
> 
> # Libraries ----------
> 
> library(network)

‘network’ 1.18.2 (2023-12-04), part of the Statnet Project
* ‘news(package="network")’ for changes since last version
* ‘citation("network")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

> library(ergm)

‘ergm’ 4.6.0 (2023-12-17), part of the Statnet Project
* ‘news(package="ergm")’ for changes since last version
* ‘citation("ergm")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

‘ergm’ 4 is a major update that introduces some backwards-incompatible
changes. Please type ‘news(package="ergm")’ for a list of major
changes.

> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> library(ergm.userterms)
Loading required package: statnet.common

Attaching package: ‘statnet.common’

The following objects are masked from ‘package:base’:

    attr, order


‘ergm.userterms’ 3.1.1 (2020-02-01), part of the Statnet Project
* ‘news(package="ergm.userterms")’ for changes since last version
* ‘citation("ergm.userterms")’ for citation information
* ‘https://statnet.org’ for help, support, and other information

NOTE: If you use custom ERGM terms based on ‘ergm.userterms’ version
prior to 3.1, you will need to perform a one-time update of the package
boilerplate files (the files that you did not write or modify) from
‘ergm.userterms’ 3.1 or later. See help('eut-upgrade') for
instructions.

> library(here)
here() starts at /oscar/home/akhann16/code/net-ergm-v4plus
> 
> 
> # Read Synthpop Data ------------------------------ 
> 
> data_path <- here("data", "synthpop-2023-10-12 12_01_32.csv")
> data <- read.csv(data_path, header=TRUE)
> glimpse(data)
Rows: 32,002
Columns: 15
$ sex                       <chr> "M", "M", "M", "M", "M", "M", "M", "M", "M",…
$ race                      <chr> "Wh", "Wh", "Wh", "Wh", "Wh", "Wh", "Wh", "W…
$ agecat                    <chr> "18-24", "18-24", "18-24", "18-24", "18-24",…
$ age_started               <int> 18, 19, 16, 19, 5, 19, 15, 17, 25, 18, 21, 1…
$ fraction_recept_sharing   <dbl> 1.00, 0.10, 0.10, 0.00, 0.00, 0.00, 0.25, 0.…
$ syringe_source            <int> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
$ daily_injection_intensity <chr> "once or twice a day", "once or twice a day"…
$ age_lb                    <int> 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, …
$ age_ub                    <int> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, …
$ age                       <int> 23, 21, 19, 20, 21, 21, 21, 21, 25, 18, 24, …
$ zipcode                   <int> 60644, 60644, 60644, 60644, 60644, 60644, 60…
$ Zip                       <int> 60644, 60644, 60644, 60644, 60644, 60644, 60…
$ lon                       <dbl> -87.76885, -87.77289, -87.76898, -87.76998, …
$ lat                       <dbl> 41.88311, 41.88240, 41.86867, 41.89144, 41.8…
$ hcv_status                <chr> "sucseptible", "sucseptible", "sucseptible",…
> 
> # Input  In/Outdegree Data ------------------------------
> # (from networks simulated form the fit, as an alternate source of targets)
> 
> inedges <- read.csv(here("data", "pplrss.csv")) #in- and out-edges
> outedges <- read.csv(here("data", "ppldss.csv"))
> negbin_indeg <- read.csv(here("data", "negbin-indeg.csv"))
> negbin_outdeg <- read.csv(here("data", "negbin-outdeg.csv"))
> inedges.data <- read.csv(file = here("data", "inedges_data.csv"), header = TRUE)
> outedges.data <- read.csv(file = here("data", "outedges_data.csv"), header = TRUE)
> 
> # Input  Degree Distribution Data ------------------------------
> 
> #degree_distribution_target_stats <- 
>  # readRDS(here("simulate-from-ergms/out/degree_distribution_target_stats.RDS"))
> #names(degree_distribution_target_stats)
> 
>  
> 
> # Initialize network ----------
> 
> n <- nrow(data)
> n0 <- network.initialize(n = n, directed = TRUE)
> 
> 
> # Compute target number of edges ----------
> 
> inedges_target <- sum(inedges.data$k * inedges.data$nbprob * n)
> outedges_target <- sum(outedges.data$k * outedges.data$nbprob * n)
> edges_target <- mean(c(inedges_target, outedges_target))
> 
> 
> 
> 
> # Assign vertex attributes to the network ----------
> 
> synthpop_cols <- colnames(data)
> set.vertex.attribute(n0, colnames(data), data)
> 
> 
> # Test assignment between `n0` vertex attributes and `synthpop_cols` ----------
> 
> ## For sex variable
> identical(n0%v%"sex", data$sex)
[1] TRUE
> table(n0%v%"sex", exclude=NULL)

    F     M 
12573 19429 
> 
> ## For all variables:
> ### get vertex attribute names from the n0 graph object
> vertex_attr_names <- list.vertex.attributes(n0)
> 
> # define a function to check if a vertex attribute is identical to the corresponding column in data
> check_identical <- function(attr_name) {
+   graph_attr <- n0 %v% attr_name
+   data_attr <- data[[attr_name]]
+   
+   return(identical(graph_attr, data_attr))
+ }
> 
> ### use sapply() to apply the check_identical() function to all vertex attribute names
> identical_results <- sapply(vertex_attr_names, check_identical)
> 
> ### print the results
> print(identical_results)
                      age                    age_lb               age_started 
                     TRUE                      TRUE                      TRUE 
                   age_ub                    agecat daily_injection_intensity 
                     TRUE                      TRUE                      TRUE 
  fraction_recept_sharing                hcv_status                       lat 
                     TRUE                      TRUE                      TRUE 
                      lon                        na                      race 
                     TRUE                     FALSE                      TRUE 
                      sex            syringe_source              vertex.names 
                     TRUE                      TRUE                     FALSE 
                      Zip                   zipcode 
                     TRUE                      TRUE 
> 
> ### check if any values other than 'vertex.names' are FALSE
> if (any(identical_results[!(names(identical_results) %in% c("vertex.names", "na"))] == FALSE)) {
+   stop("Error: Some vertex attributes do not match the corresponding columns in data.")
+ } else {
+   cat("Test passed: All relevant vertex attributes match the corresponding columns in data.\n")
+ }
Test passed: All relevant vertex attributes match the corresponding columns in data.
> 
> 
> # Recode to add new variables to dataset ------------------------------
> 
> # young (< 26, to be set = 1) vs old (>= 26, to be set = 0) 
> age.cat <- n0 %v% "age"
> age.cat.df <- as.data.frame(age.cat)
> age.cat.df <-  
+   mutate(age.cat, young = ifelse(age.cat <26, 1, 0), .data = age.cat.df)
> xtabs(~age.cat+young, data = age.cat.df)
       young
age.cat    0    1
     18    0  471
     19    0  695
     20    0  854
     21    0 1053
     22    0 1142
     23    0 1286
     24    0 1561
     25    0  739
     26  787    0
     27  910    0
     28  946    0
     29 1018    0
     30 1024    0
     31 1027    0
     32 1119    0
     33 1110    0
     34 1117    0
     35  576    0
     36  585    0
     37  584    0
     38  617    0
     39  611    0
     40  676    0
     41  670    0
     42  732    0
     43  727    0
     44  719    0
     45  240    0
     46  232    0
     47  261    0
     48  253    0
     49  238    0
     50  225    0
     51  228    0
     52  233    0
     53  250    0
     54  235    0
     55  224    0
     56  231    0
     57  225    0
     58  269    0
     59  230    0
     60  258    0
     61  238    0
     62  245    0
     63  251    0
     64  252    0
     65  253    0
     66  212    0
     67  258    0
     68  231    0
     69  238    0
     70  229    0
     71  235    0
     72  219    0
     73  243    0
     74  263    0
     75  271    0
     76  243    0
     77  213    0
     78  256    0
     79  240    0
     80  224    0
> 
> # recode race variable to set ordering of categories
> race <- n0 %v% "race"
> race.num <- recode(race, 
+                    Wh = 1, Bl = 2,
+                    Hi = 3, Ot = 4)
> 
> # Initialize network and add attributes ----------
> 
> n0 %v% "young" <- age.cat.df$young
> n0 %v% "race.num" <- race.num
> 
> 
> # Generate target statistics from meta-mixing data ----------
> 
> ## gender 
> ### mixing information from meta-analysis of sathcap AND socnet
> edges.male.end <- mean(c(0.58, 0.59))
> edges.female.end <- mean(c(0.40, 0.41))
> 
> male.pctmale <- 0.53 
> male.pctfemale <- 0.47
> female.pctmale <- 0.75
> female.pctfemale <- 0.22
> 
> ### set gender targets 
> tgt.male.pctmale <- edges_target*edges.male.end*male.pctmale
> tgt.male.pctfemale <- edges_target*edges.male.end*male.pctfemale
> tgt.female.pctmale <- edges_target*edges.female.end*female.pctmale  
> tgt.female.pctfemale <- edges_target*edges.female.end*female.pctfemale
> 
> ## young (1=young; 0=old)
> ### mixing information from meta-analysis of sathcap AND socnet
> edges.young.end <- 0.10
> edges.old.end <- 0.90
>   
> young.pctyoung <- 0.60
> young.pctold <- 0.40
> old.pctyoung <- 0.14
> old.pctold <- 0.86
>  
> ## set young targets from meta data
> tgt.young.pctyoung <- edges_target * edges.young.end * young.pctyoung
> tgt.young.pctold <- edges_target * edges.young.end * young.pctold
> tgt.old.pctyoung <- edges_target * edges.old.end * old.pctyoung
> tgt.old.pctold <- edges_target * edges.old.end * old.pctold
> 
> 
> ## chicago
> ### mixing information from meta-analysis of sathcap AND socnet
> 
> edges.chicago.end <- 0.87
> edges.nonchicago.end <- 0.13
> 
> chicago.pctchicago <- 0.67
> chicago.pctnonchicago <- 0.30
> nonchicago.pctchicago <- 0.60
> nonchicago.pctnonchicago <- 0.40
> 
> 
> # ### mixing from simulation
> # chicago.mm <- mixingmatrix(sim, "chicago") #fem=1, chicago=2
> # from.nonchicago <- sum(chicago.mm$matrix[,1])
> # from.chicago <- sum(chicago.mm$matrix[,2])
> # 
> # ### set chicago targets from sathcap
> # tgt.chicago.pctchicago <- from.chicago*chicago.pctchicago
> # tgt.chicago.pctnonchicago <- from.chicago*chicago.pctnonchicago
> # tgt.nonchicago.pctchicago <- from.nonchicago*chicago.pctchicago
> # tgt.nonchicago.pctnonchicago <- from.nonchicago*chicago.pctnonchicago
> 
> 
> ## race (1=Black, 2=hispani,3=other, 4=white)
> 
> table(n0 %v% "race.num", exclude=NULL) # will be sorted as per 1=W, 2=B, 3=H, 4=O

    1     2     3     4 
15403  8317  6853  1429 
> 
> pct_to_white	<- mean(c(0.30, 0.31))
> pct_to_black	<- mean(c(0.41,	0.42))
> pct_to_hispanic	<- mean(c(0.21, 0.22))
> pct_to_other	<- mean(c(0.04,	0.05))
> 
> race.w.w <- 0.73
> race.b.w <- 0.10
> race.h.w <- 0.12
> race.o.w <- 0.04
> race.w.b <- 0.10
> race.b.b <- 0.83
> race.h.b <- 0.06
> race.o.b <- 0.01
> race.w.h <- 0.21
> race.b.h <- 0.15
> race.h.h <- 0.62
> race.o.h <- 0.02
> race.w.o <- 0.43
> race.b.o <- 0.21
> race.h.o <- 0.22
> race.o.o <- 0.14
> 
> target.w.w <- edges_target * pct_to_white * race.w.w
> target.b.w <- edges_target * pct_to_white * race.b.w
> target.h.w <- edges_target * pct_to_white * race.h.w
> target.o.w <- edges_target * pct_to_white * race.o.w
> 
> target.w.b <- edges_target * pct_to_black * race.w.b
> target.b.b <- edges_target * pct_to_black * race.b.b
> target.h.b <- edges_target * pct_to_black * race.h.b
> target.o.b <- edges_target * pct_to_black * race.o.b
> 
> target.w.h <- edges_target * pct_to_hispanic * race.w.h
> target.b.h <- edges_target * pct_to_hispanic * race.b.h
> target.h.h <- edges_target * pct_to_hispanic * race.h.h
> target.o.h <- edges_target * pct_to_hispanic * race.o.h
> 
> target.w.o <- edges_target * pct_to_other * race.w.o
> target.b.o <- edges_target * pct_to_other * race.b.o
> target.h.o <- edges_target * pct_to_other * race.h.o
> target.o.o <- edges_target * pct_to_other * race.o.o
> 
> ## degree distributions
> inedges <- inedges %>% 
+   mutate(n_nodes = n*nbprob)
> outedges <- outedges %>% 
+   mutate(n_nodes = n*nbprob)
> 
> negbin_inedges <- negbin_indeg %>% 
+   mutate(n_nodes = n*nbprob)
> negbin_outedges <- negbin_outdeg %>% 
+   mutate(n_nodes = n*nbprob)
> 
> ## distance term
> 
> dist.prop.distribution <- c(15.7, 35.1, 24.1, 22)/100
> dist.nedge.distribution <- edges_target*dist.prop.distribution
> 
> # Fit ERGM (with SATHCAP mixing) ----------
> 
> deg.terms <- 0:3
> indeg.terms <- 0:1  
> 
> dist.terms <- 1:3 #fourth is left out
> #dist.terms <- 3 #only try the third term
> 
> # Specify initial coefs ----------
> 
> initial_coeffs <- c("edges" = -9.319, 
+                     "mix.race.num.4.1" = 0.16349, 
+                     "odegree0" = -0.1386842, 
+                     "mix.race.num.2.4" = 0.13625, 
+                     "mix.race.num.4.4" = 0.20408)
> 
> fit.metadata.mixing <-
+   ergm(
+     n0 ~
+       edges + 
+       nodemix("sex", levels2=-1)+
+       nodemix("young", levels2=-1)+
+       nodemix("race.num", levels2=-1)+
+       idegree(indeg.terms)+
+       odegree(deg.terms)+
+       dist(dist.terms),
+     target.stats = 
+     c(
+       edges_target,
+       c(tgt.female.pctmale, tgt.male.pctfemale, tgt.male.pctmale),           
+       c(tgt.old.pctyoung, tgt.young.pctold, tgt.young.pctyoung),
+       c(target.b.w, target.h.w, target.o.w,
+         target.w.b, target.b.b, target.h.b, target.o.b,
+         target.w.h, target.b.h, target.h.h, target.o.h,
+         target.w.o, target.b.o, target.h.o, target.o.o),
+       c(negbin_inedges$n_nodes[c(indeg.terms+1)]),
+       c(outedges$n_nodes[c(deg.terms+1)]),
+       c(dist.nedge.distribution[dist.terms])
+     ),
+     eval.loglik = FALSE    
+     )
Unable to match target stats. Using MCMLE estimation.
Starting maximum pseudolikelihood estimation (MPLE):
Obtaining the responsible dyads.
Evaluating the predictor and response matrix.
Maximizing the pseudolikelihood.
Finished MPLE.
Starting Monte Carlo maximum likelihood estimation (MCMLE):
Iteration 1 of at most 60:
Optimizing with step length 0.0018.
The log-likelihood improved by 2.0287.
Estimating equations are not within tolerance region.
Iteration 2 of at most 60:
Optimizing with step length 0.0025.
The log-likelihood improved by 2.0910.
Estimating equations are not within tolerance region.
Iteration 3 of at most 60:
Optimizing with step length 0.0030.
The log-likelihood improved by 2.2318.
Estimating equations are not within tolerance region.
Iteration 4 of at most 60:
Optimizing with step length 0.0035.
The log-likelihood improved by 2.3875.
Estimating equations are not within tolerance region.
Iteration 5 of at most 60:
Optimizing with step length 0.0044.
The log-likelihood improved by 2.7511.
Estimating equations are not within tolerance region.
Iteration 6 of at most 60:
Optimizing with step length 0.0031.
The log-likelihood improved by 1.9606.
Estimating equations are not within tolerance region.
Iteration 7 of at most 60:
Optimizing with step length 0.0037.
The log-likelihood improved by 2.0892.
Estimating equations are not within tolerance region.
Iteration 8 of at most 60:
Optimizing with step length 0.0042.
The log-likelihood improved by 1.9570.
Estimating equations are not within tolerance region.
Iteration 9 of at most 60:
Optimizing with step length 0.0016.
The log-likelihood improved by 1.6974.
Estimating equations are not within tolerance region.
Iteration 10 of at most 60:
Optimizing with step length 0.0051.
The log-likelihood improved by 2.3723.
Estimating equations are not within tolerance region.
Iteration 11 of at most 60:
Optimizing with step length 0.0049.
The log-likelihood improved by 2.1644.
Estimating equations are not within tolerance region.
Iteration 12 of at most 60:
Optimizing with step length 0.0062.
The log-likelihood improved by 2.2364.
Estimating equations are not within tolerance region.
Iteration 13 of at most 60:
Optimizing with step length 0.0057.
The log-likelihood improved by 2.0219.
Estimating equations are not within tolerance region.
Iteration 14 of at most 60:
Optimizing with step length 0.0056.
The log-likelihood improved by 2.1418.
Estimating equations are not within tolerance region.
Iteration 15 of at most 60:
Optimizing with step length 0.0063.
The log-likelihood improved by 2.2051.
Estimating equations are not within tolerance region.
Iteration 16 of at most 60:
Optimizing with step length 0.0071.
The log-likelihood improved by 2.4038.
Estimating equations are not within tolerance region.
Iteration 17 of at most 60:
Optimizing with step length 0.0070.
The log-likelihood improved by 2.4521.
Estimating equations are not within tolerance region.
Iteration 18 of at most 60:
Optimizing with step length 0.0070.
The log-likelihood improved by 2.4996.
Estimating equations are not within tolerance region.
Iteration 19 of at most 60:
Optimizing with step length 0.0056.
The log-likelihood improved by 2.1104.
Estimating equations are not within tolerance region.
Iteration 20 of at most 60:
Optimizing with step length 0.0080.
The log-likelihood improved by 2.3157.
Estimating equations are not within tolerance region.
Iteration 21 of at most 60:
Optimizing with step length 0.0075.
The log-likelihood improved by 2.2715.
Estimating equations are not within tolerance region.
Iteration 22 of at most 60:
Optimizing with step length 0.0085.
The log-likelihood improved by 2.4852.
Estimating equations are not within tolerance region.
Iteration 23 of at most 60:
Optimizing with step length 0.0085.
The log-likelihood improved by 2.1389.
Estimating equations are not within tolerance region.
Iteration 24 of at most 60:
Optimizing with step length 0.0080.
The log-likelihood improved by 1.9051.
Estimating equations are not within tolerance region.
Iteration 25 of at most 60:
Optimizing with step length 0.0086.
The log-likelihood improved by 2.1812.
Estimating equations are not within tolerance region.
Iteration 26 of at most 60:
Optimizing with step length 0.0095.
The log-likelihood improved by 2.5913.
Estimating equations are not within tolerance region.
Iteration 27 of at most 60:
Optimizing with step length 0.0103.
The log-likelihood improved by 2.3993.
Estimating equations are not within tolerance region.
Iteration 28 of at most 60:
Optimizing with step length 0.0093.
The log-likelihood improved by 2.0948.
Estimating equations are not within tolerance region.
Iteration 29 of at most 60:
Optimizing with step length 0.0092.
The log-likelihood improved by 2.2637.
Estimating equations are not within tolerance region.
Iteration 30 of at most 60:
Optimizing with step length 0.0102.
The log-likelihood improved by 2.3116.
Estimating equations are not within tolerance region.
Iteration 31 of at most 60:
Optimizing with step length 0.0102.
The log-likelihood improved by 2.3123.
Estimating equations are not within tolerance region.
Iteration 32 of at most 60:
Optimizing with step length 0.0111.
The log-likelihood improved by 2.3542.
Estimating equations are not within tolerance region.
Iteration 33 of at most 60:
Optimizing with step length 0.0118.
The log-likelihood improved by 2.7439.
Estimating equations are not within tolerance region.
Iteration 34 of at most 60:
Optimizing with step length 0.0118.
The log-likelihood improved by 2.4736.
Estimating equations are not within tolerance region.
Iteration 35 of at most 60:
Optimizing with step length 0.0133.
The log-likelihood improved by 2.7172.
Estimating equations are not within tolerance region.
Iteration 36 of at most 60:
Optimizing with step length 0.0122.
The log-likelihood improved by 2.0707.
Estimating equations are not within tolerance region.
Iteration 37 of at most 60:
Optimizing with step length 0.0148.
The log-likelihood improved by 3.1088.
Estimating equations are not within tolerance region.
Iteration 38 of at most 60:
Optimizing with step length 0.0109.
The log-likelihood improved by 2.0835.
Estimating equations are not within tolerance region.
Iteration 39 of at most 60:
Optimizing with step length 0.0137.
The log-likelihood improved by 2.3063.
Estimating equations are not within tolerance region.
Iteration 40 of at most 60:
Optimizing with step length 0.0128.
The log-likelihood improved by 2.1913.
Estimating equations are not within tolerance region.
Iteration 41 of at most 60:
Optimizing with step length 0.0130.
The log-likelihood improved by 2.4275.
Estimating equations are not within tolerance region.
Iteration 42 of at most 60:
Optimizing with step length 0.0146.
The log-likelihood improved by 2.3817.
Estimating equations are not within tolerance region.
Iteration 43 of at most 60:
Optimizing with step length 0.0154.
The log-likelihood improved by 2.4526.
Estimating equations are not within tolerance region.
Iteration 44 of at most 60:
Optimizing with step length 0.0156.
The log-likelihood improved by 2.6434.
Estimating equations are not within tolerance region.
Iteration 45 of at most 60:
Optimizing with step length 0.0158.
The log-likelihood improved by 2.3314.
Estimating equations are not within tolerance region.
Iteration 46 of at most 60:
Optimizing with step length 0.0155.
The log-likelihood improved by 2.2120.
Estimating equations are not within tolerance region.
Iteration 47 of at most 60:
Optimizing with step length 0.0164.
The log-likelihood improved by 2.4922.
Estimating equations are not within tolerance region.
Iteration 48 of at most 60:
Optimizing with step length 0.0166.
The log-likelihood improved by 2.6145.
Estimating equations are not within tolerance region.
Iteration 49 of at most 60:
Optimizing with step length 0.0166.
The log-likelihood improved by 2.0318.
Estimating equations are not within tolerance region.
Iteration 50 of at most 60:
Optimizing with step length 0.0191.
The log-likelihood improved by 2.7058.
Estimating equations are not within tolerance region.
Iteration 51 of at most 60:
Optimizing with step length 0.0191.
The log-likelihood improved by 2.5909.
Estimating equations are not within tolerance region.
Iteration 52 of at most 60:
Optimizing with step length 0.0226.
The log-likelihood improved by 3.6500.
Estimating equations are not within tolerance region.
Iteration 53 of at most 60:
Optimizing with step length 0.0190.
The log-likelihood improved by 2.3590.
Estimating equations are not within tolerance region.
Iteration 54 of at most 60:
Optimizing with step length 0.0186.
The log-likelihood improved by 2.3496.
Estimating equations are not within tolerance region.
Iteration 55 of at most 60:
Optimizing with step length 0.0200.
The log-likelihood improved by 2.3446.
Estimating equations are not within tolerance region.
Iteration 56 of at most 60:
Optimizing with step length 0.0204.
The log-likelihood improved by 2.2152.
Estimating equations are not within tolerance region.
Iteration 57 of at most 60:
Optimizing with step length 0.0199.
The log-likelihood improved by 2.3374.
Estimating equations are not within tolerance region.
Iteration 58 of at most 60:
Optimizing with step length 0.0206.
The log-likelihood improved by 2.2672.
Estimating equations are not within tolerance region.
Iteration 59 of at most 60:
Optimizing with step length 0.0231.
The log-likelihood improved by 2.7188.
Estimating equations are not within tolerance region.
Iteration 60 of at most 60:
Optimizing with step length 0.0215.
The log-likelihood improved by 2.2889.
Estimating equations are not within tolerance region.
MCMLE estimation did not converge after 60 iterations. The estimated coefficients may not be accurate. Estimation may be resumed by passing the coefficients as initial values; see 'init' under ?control.ergm for details.
Finished MCMLE.
This model was fit using MCMC.  To examine model diagnostics and check
for degeneracy, use the mcmc.diagnostics() function.
>   
>   
> 
> 
> proc.time()
     user    system   elapsed 
84444.924    12.449 84770.550 
